\section{Introduction}

[scientific applications and it's workflow]


Workflow is typically defined as a sequence of operations or tasks needed to manage a buiiness or computational activity, the scientific workflow tools aims to support the execution of scientific applications mainly including tasks such as scientific simulations, analysis and visualization etc. one character is the large data exchange between those applications. In-situ workflow could be composed by exchanging those intermediate data by memory/storage hierarchy and network of a high-performance computing (HPC) \cite{deelman2018future}. Those intermediate data can be fully used to guide the excution of the scientific workflow.  


[general description about the question we want to solve]

According to the communication model between tasks, the scientific workflow could be divided into integrated workflow and connected workflow.\cite{dreher2017situ}. Integrated workflow run all tasks in one MPI program and Connected workflow coordinate tasks separated into different programs not sharing common communicator. One advantage for connected workflow is customizing the occasion to start the tasks based on the influential events in workflow.

Typical workflow engine follows predefined DAG (Directed Acyclic Graph) to start every tasks. Every node in DAG represent a task and every edge represent the dependency between task. According to the method to establish the dependency, the workflow could be divided into control-driven workflows and data-driven workflows\cite{shields2007control}. Data driven is constructed based on the data dependency between two tasks, namely the output of first task is the input for second task. Compare with the control-driven flow (Task A must run after Task B), the data driven flow provide a view to describe the task dependency in much finer granularity way.

[challenge of scientific workflow for data driven dependency]

One scenario in complex scientific application is that the dependency is constructed according to the status of task itself and the contents of intermediated data. For example, when the task fail or intermediate data satisfy some conditions, extra tasks will be triggered. 

Some project have already provide the solution for those dependency construction such as Decaf \cite{dreher2017decaf} and Argo\cite{perarnau2015distributed} but they mechanism of how to construct and control fine granularity depedency is not fully explored. We leverage the dynamicity of the connected workflow based on the event driven pub-sub architecture and let user describe and subscribe their interested events for every task. When those event based on data content happens in workflow, the related task will be triggered based on those events. 

slurm is the common tool For HPC users,  even if it provide the strigger function to assign a trigger command when some events happens\cite{slurmtrigure}, but only the user with root permission could use the slurm trigger function and the types of the event is also limited and could not be customized.

advantages to use the container Singularity(1 convenience for development and production 2 provide more dynamic way to monitor the runtime of the process and publish specific status by event message)

[our solution and main contribution]

There are several for communication between pub-sub system based on space, time synchronization\cite{eugster2003many}. We leverage these advantages by using it to express data dependency between workflow tasks. In this paper, we mainly make the following contributions:(planned)

1.Provide the expression model to let user describe the event and the logic to judge how this event will happen.

2.Implement a event driven pub-sub system which could start the task at the occasion of the subscribed event happen in system

3.Provide the mechanism to acquire the events from the running tasks status and the intermediate data of the workflow.

4.Validate the effectiveness of workflow tools in different use scenarios.

dynamic register the event in running time is also another contribution (the function of unsubscribe could also be added into the system)



The rest paper is organized as follows, Section \uppercase\expandafter{\romannumeral2} introduce the background and the motivations for event-driven workflow, Section \uppercase\expandafter{\romannumeral3} introduce the design thoughts of the framework. Section \uppercase\expandafter{\romannumeral4} provide the details for implementation. Section \uppercase\expandafter{\romannumeral5} presents the experiments to show the performance of the framework. Section \uppercase\expandafter{\romannumeral6} introduce the conclusion and the future work of the paper.
